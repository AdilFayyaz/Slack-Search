# requirements.py (if running in standalone script)
# You might want to run these in a terminal or manually manage with requirements.txt
# !pip install uv
# !uv pip install fastmcp
# !pip install scikit-learn

import os
import xml.etree.ElementTree as ET
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from fastmcp import FastMCP
import math
import time

# -------------------------------
# Dataset Import
# -------------------------------

def parse_all_communities(base_dir):
    dataset = defaultdict(lambda: defaultdict(dict))  # community -> year -> conversations
    for community in os.listdir(base_dir):
        community_path = os.path.join(base_dir, community)
        if not os.path.isdir(community_path):
            continue

        for year in os.listdir(community_path):
            year_path = os.path.join(community_path, year)
            if not os.path.isdir(year_path):
                continue

            for filename in os.listdir(year_path):
                if filename.endswith('.xml'):
                    file_path = os.path.join(year_path, filename)
                    print(f"Parsing: {file_path}")
                    tree = ET.parse(file_path)
                    root = tree.getroot()
                    conversations = defaultdict(list)
                    for msg in root.findall('message'):
                        conv_id = msg.attrib.get('conversation_id')
                        ts = msg.find('ts').text
                        user = msg.find('user').text
                        text = msg.find('text').text
                        conversations[conv_id].append({'ts': ts, 'user': user, 'text': text})
                    dataset[community][year].update(conversations)
    return dataset

# all_data = parse_all_communities('Slack-dataset/data')
all_data = parse_all_communities('/Users/.../data')
# -------------------------------
# Prepare Documents
# -------------------------------

docs = []
meta = []

for community, years in all_data.items():
    for year, convs in years.items():
        for conv_id, msgs in convs.items():
            sorted_msgs = sorted(msgs, key=lambda x: x.get('ts', ''))
            texts = [m['text'].strip() for m in sorted_msgs if m.get('text')]
            full_text = ' '.join(texts)

            if full_text:
                docs.append(full_text)
                meta.append({
                    'community': community,
                    'year': year,
                    'conversation_id': conv_id
                })

# -------------------------------
# Vectorization
# -------------------------------

mcp = FastMCP(
    name="search_slack",
    host="127.0.0.1",
    port=5000,
    timeout=20
)

vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(docs)

# -------------------------------
# MCP Tools
# -------------------------------

@mcp.tool()
def search_slack(query: str, community: str = None, year: str = None, top_n: int = 5):
    """Search Slack conversations based on a query"""
    filtered_docs = docs
    filtered_meta = meta
    if community:
        filtered_docs, filtered_meta = zip(*[
            (doc, meta)
            for doc, meta in zip(docs, meta)
            if meta['community'] == community
        ])

    if year:
        filtered_docs, filtered_meta = zip(*[
            (doc, meta)
            for doc, meta in zip(filtered_docs, filtered_meta)
            if meta['year'] == year
        ])

    matrix = tfidf_matrix if not (community or year) else vectorizer.transform(filtered_docs)
    query_vec = vectorizer.transform([query])
    similarities = cosine_similarity(query_vec, matrix).flatten()
    top_indices = similarities.argsort()[-top_n:][::-1]

    results = []
    for idx in top_indices:
        result = {
            "score": round(similarities[idx], 4),
            "content": filtered_docs[idx],
            "metadata": filtered_meta[idx]
        }
        results.append(result)
    return results

@mcp.tool()
def summarize_conversations(conversations: list) -> str:
    """Summarize retrieved conversations"""
    summary = "\n".join([
        f"Community: {conv['metadata']['community']}, Year: {conv['metadata']['year']}\n{conv['content'][:300]}..."
        for conv in conversations
    ])
    return f"Summary of top conversations:\n{summary}"


# -------------------------------
# Run MCP
# -------------------------------

if __name__ == "__main__":
    try:
        print("Starting MCP server 'search-slack' on 127.0.0.1:5000")
        # Use this approach to keep the server running
        mcp.run()
    except Exception as e:
        print(f"Error: {e}")
        # Sleep before exiting to give time for error logs
        time.sleep(5)